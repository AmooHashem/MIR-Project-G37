{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b8366ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hazm datasets textblob sklearn nltk pyg-nightly tabulate torch sklearn sentencepiece transformers sentence-transformers pandas faiss-cpu numpy folium streamlit datasets pickle5 sklearn hazm nltk tqdm networkx==2.6.3 spacy en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680e8516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import socket, time\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer, SimpleHTTPRequestHandler, test\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, BigBirdModel, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download(\"popular\")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "# import faiss\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n",
    "from tabulate import tabulate\n",
    "from textblob import TextBlob\n",
    "from textblob.en import Spelling        \n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from hazm import *\n",
    "random.seed(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c0c3a",
   "metadata": {},
   "source": [
    "# HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c95de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f75dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw3 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw3.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw3['pos_or_neg'].replace({4: 1}, inplace=True)\n",
    "DOC_NUMBER = len(df_hw3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7802ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw3 = df_hw3.sample(n=max_dataset_items)\n",
    "DOC_NUMBER = max_dataset_items\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def normalize_sent(text):\n",
    "    text = normalize_text(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda x: normalize_text(x))\n",
    "# df_hw3['passage'] = df_hw3['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "df_3 = pd.DataFrame()\n",
    "df_3[\"Text\"] = df_hw3['passage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbbe1a9",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8393c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_hw3 = TfidfVectorizer()\n",
    "tfidf_df = vectorizer_hw3.fit_transform(df_3['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10d324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_tf_idf(user_query, output_size = 10):\n",
    "    user_query = normalize_sent(user_query)\n",
    "\n",
    "    input_tfidf = vectorizer_hw3.transform([user_query]).toarray()[0]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(DOC_NUMBER):\n",
    "        similarity = tfidf_df[i].dot(input_tfidf)[0]\n",
    "        scores.append((i, similarity))\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df_3['Text'].iloc[scores[i][0]]])\n",
    "    return [result, 'tf-idf']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727998b",
   "metadata": {},
   "source": [
    "## Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810656bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_boolean_df():\n",
    "  '''\n",
    "  all_words = list(itertools.chain(*[[t for t in message.split()] for message in df_hw3[\"Text\"]]))\n",
    "\n",
    "  empty_dataframe_input = {}\n",
    "  number_of_tweets = len(df_hw3[\"Text\"])\n",
    "  for word in all_words:\n",
    "    empty_dataframe_input[word] = [0 for i in range(0, number_of_tweets)]\n",
    "\n",
    "  boolean_df = pd.DataFrame(empty_dataframe_input)\n",
    "  for i in range(len(df_hw3[\"Text\"])):\n",
    "    words = word_tokenize(df_hw3[\"Text\"].iloc[i])\n",
    "    for word in words:\n",
    "      boolean_df.at[i, word] = 1\n",
    "  return boolean_df\n",
    "  '''\n",
    "  print()\n",
    "boolean_df = create_boolean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1566c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_boolean_embeddings(user_query, output_size = 10):\n",
    "    '''\n",
    "    sent = normalize_sent(user_query)\n",
    "\n",
    "    out = {}\n",
    "    for i in range(len(df_hw3[\"Text\"])):\n",
    "        out[i] = 0\n",
    "    out = pd.Series(data=out)\n",
    "    for word in sent:\n",
    "        if word in boolean_df.columns:\n",
    "            out = out + boolean_df[word]\n",
    "    out = out.sort_values(ascending=False)\n",
    "\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df_hw3[\"Text\"].iloc[out.index[i]]])\n",
    "\n",
    "    return [result, 'boolean']\n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, 2*output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results[0:output_size], 'boolean']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75f453",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1843a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, model, index, num_results=10):\n",
    "    vector = model.encode(list(query))\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
    "    return D, I\n",
    "\n",
    "def id2details(df, I, column):\n",
    "    return [list(df[df.id == idx][column]) for idx in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba87223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "embeddings = model.encode(df.Text.to_list(), show_progress_bar=True)\n",
    "print(f'Shape of the vectorised abstract: {embeddings[0].shape}')\n",
    "\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index = faiss.IndexIDMap(index)\n",
    "index.add_with_ids(embeddings, df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")\n",
    "df.iloc[400, 1]\n",
    "D, I = index.search(np.array([embeddings[1]]), k=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "print(id2details(df, I, 'Text'))\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a143b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_transformers(user_query, output_size = 10):\n",
    "    '''\n",
    "    user_query = normalize_sent(user_query)\n",
    "    D, I = vector_search([user_query], model, index, num_results=output_size)\n",
    "    #print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "    return[id2details(df, I, 'Text'), 'transformer']\n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, output_size)[0]\n",
    "    results2 = search_with_tf_idf(user_query, output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results2[0:output_size//2] + results[0:output_size//2], 'transformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd107a",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc1447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = [x.split() for x in df_3['Text']]\n",
    "\n",
    "#ftmodel = FastText(sentences=data, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0a51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_fastText_embeddings(user_query, output_size = 10):\n",
    "    '''\n",
    "    user_query = normalize_sent(user_query)\n",
    "    user_query = user_query.split()\n",
    "    scores = []\n",
    "    for document in df[\"Text\"]:\n",
    "        if document != '':\n",
    "            similarity = ftmodel.wv.n_similarity(user_query, document.split())\n",
    "            scores.append((document, similarity))\n",
    "\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([scores[i][0]])\n",
    "\n",
    "    return [result, 'fastText']\n",
    "    \n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results, 'fastText']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b53490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_mudoles = {\n",
    "    'boolean': search_with_boolean_embeddings,\n",
    "    'tf_idf': search_with_tf_idf, \n",
    "    'transformer': search_with_transformers, \n",
    "    'fastText': search_with_fastText_embeddings\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c07fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hw3(query, modul_type, output_size=10):\n",
    "    return search_mudoles[modul_type](query, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1dcaa1",
   "metadata": {},
   "source": [
    "# Query Expantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4b1c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model():\n",
    "    # Create a placeholder for model\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for sentence in df_3['Text'] :\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split(' ')\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model[(w1, w2)][w3] += 1\n",
    "        \n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model2[w1][w2] += 1\n",
    "            \n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1_w2 in model:\n",
    "        total_count = float(sum(model[w1_w2].values()))\n",
    "        for w3 in model[w1_w2]:\n",
    "            model[w1_w2][w3] /= total_count\n",
    "    \n",
    "    for w1 in model2:\n",
    "        total_count = float(sum(model2[w1].values()))\n",
    "        for w2 in model2[w1]:\n",
    "            model2[w1][w2] /= total_count\n",
    "    \n",
    "    return model, model2\n",
    "\n",
    "def create_spell_checker_input():\n",
    "    oneString = ''\n",
    "    for sentence in df_3['Text'] :\n",
    "        textToLower = sentence.lower()\n",
    "        words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
    "        oneString += \" \".join(words)  \n",
    "    pathToFile = \"textblobtrain.txt\"                              # The path we want to store our stats file at\n",
    "    spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
    "    spelling.train(oneString, pathToFile)                 # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1f5d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction_defualt(text):\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    return textCorrected\n",
    "\n",
    "def spell_correction(text):\n",
    "    pathToFile = \"textblobtrain.txt\" \n",
    "    spelling = Spelling(path = pathToFile)\n",
    "    words = text.split()\n",
    "    corrected = \" \"\n",
    "    for i in words :\n",
    "        corrected = corrected +\" \"+ spelling.suggest(i)[0][0] # Spell checking word by word\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a0f8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model3, language_model2 = create_language_model()\n",
    "create_spell_checker_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc1e6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(w):\n",
    "    return w.lower() if w is not None else None\n",
    "\n",
    "def get_next_word_list(model, w1, w2):\n",
    "    w1 = to_lower(w1)\n",
    "    w2 = to_lower(w2)\n",
    "    return sorted(dict(model[w1, w2]).items(), key=lambda x:-1*x[1])\n",
    "\n",
    "def expand_with_language_model(query):\n",
    "    query_list = [str(query)]\n",
    "    sentence = query\n",
    "    sentence = sentence.split(' ')\n",
    "    indx = 0\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        sent = sentence.copy()[:indx+2]\n",
    "        a, b = w1, w2\n",
    "        for i in range(len(sentence) - indx):\n",
    "            c_list = get_next_word_list(language_model3, a, b)\n",
    "\n",
    "            try:\n",
    "                if c_list[0][0] is None:\n",
    "                    break\n",
    "                sent.append(c_list[0][0])\n",
    "                sent2 = sent.copy()\n",
    "                sent2.extend(sentence[len(sent):])\n",
    "                query_list.append(' '.join(sent2))\n",
    "                a, b = b, c_list[0][0]\n",
    "            except:\n",
    "                break\n",
    "        indx += 1\n",
    "\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39890651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(model, sentence):\n",
    "    epsilone = 1e-10\n",
    "    out = 0\n",
    "    indx = 0\n",
    "    sentence = sentence.split(' ')\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        w1, w2 = to_lower(w1), to_lower(w2)\n",
    "        p = dict(model[w1])[w2] if w2 in model[w1].keys() else 0\n",
    "        p = math.log(p + epsilone)\n",
    "        out += p\n",
    "        indx += 1\n",
    "    return out/indx\n",
    "\n",
    "def expand_query(query, max_out = 7):\n",
    "    query_list = []\n",
    "\n",
    "    query_list.extend(expand_with_language_model(query))\n",
    "    query_list.extend(expand_with_language_model(spell_correction(query)))\n",
    "    query_list.extend(expand_with_language_model(spell_correction_defualt(query)))\n",
    "    l = []\n",
    "    for q in query_list:\n",
    "        l.append((get_p_value(language_model2, q), q.strip()))\n",
    "    l = sorted(l, key=lambda x:-1*x[0])[:max_out]\n",
    "    return [q for _, q in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06396d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hw3_results(query, query_type, queryExp):\n",
    "    expantion_results = []\n",
    "    query_results = search_hw3(query, query_type)[0]\n",
    "    if queryExp:\n",
    "        expantion_results = expand_query(query, 10)\n",
    "    \n",
    "    return query_results, expantion_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4162d902",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_hw3_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgjhhjv. sd s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mget_hw3_results\u001b[0;34m(query, query_type, queryExp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hw3_results\u001b[39m(query, query_type, queryExp):\n\u001b[1;32m      2\u001b[0m     expantion_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     query_results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_hw3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m queryExp:\n\u001b[1;32m      5\u001b[0m         expantion_results \u001b[38;5;241m=\u001b[39m expand_query(query, \u001b[38;5;241m10\u001b[39m)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36msearch_hw3\u001b[0;34m(query, modul_type, output_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_hw3\u001b[39m(query, modul_type, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msearch_mudoles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodul_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36msearch_with_transformers\u001b[0;34m(user_query, output_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_with_transformers\u001b[39m(user_query, output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    user_query = normalize_sent(user_query)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    D, I = vector_search([user_query], model, index, num_results=output_size)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    #print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    return[id2details(df, I, 'Text'), 'transformer']\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_with_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      9\u001b[0m     results2 \u001b[38;5;241m=\u001b[39m search_with_tf_idf(user_query, output_size)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(results)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36msearch_with_tf_idf\u001b[0;34m(user_query, output_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_with_tf_idf\u001b[39m(user_query, output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      2\u001b[0m     user_query \u001b[38;5;241m=\u001b[39m normalize_sent(user_query)\n\u001b[0;32m----> 4\u001b[0m     input_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(DOC_NUMBER):\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2101\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[1;32m   2085\u001b[0m \n\u001b[1;32m   2086\u001b[0m \u001b[38;5;124;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mtransform(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1379\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1381\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "get_hw3_results('gjhhjv. sd s', 'transformer', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566a37d",
   "metadata": {},
   "source": [
    "# HW4\n",
    "## HW4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c69733ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw4 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw4.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw4['pos_or_neg'].replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "586bd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw4 = df_hw4.sample(n=max_dataset_items)\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: normalize_text(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85974545",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "### A) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "306c75e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"logistic regression's f1_macro score: 0.47570980825958703\",\n",
       " \"logistic regression's f1_micro data score: 0.818\",\n",
       " \"logistic regression's confusion matrix:\",\n",
       " '[813   3]',\n",
       " '[179   5]',\n",
       " \"logistic regression's accuracy is: 81.8%\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words = None, norm='l2')\n",
    "x_tfidf_hw4 = vectorizer.fit_transform(df_hw4.passage)\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf_hw4, df_hw4.pos_or_neg, test_size = 0.1, random_state = 1)\n",
    "regressor_hw4 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(x_train_lr, y_train_lr)\n",
    "\n",
    "def lr_results():\n",
    "    metrics = []\n",
    "    \n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='macro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_macro score: {score}')\n",
    "\n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='micro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_micro data score: {score}')\n",
    "\n",
    "    results = confusion_matrix(y_test_lr, y_predicted)\n",
    "    metrics.append(f'logistic regression\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    \n",
    "\n",
    "    y_test1 = [y for y in y_test_lr]\n",
    "    y_pred1 = [y for y in y_predicted]\n",
    "    counter = 0\n",
    "    for i in range(len(y_test_lr)):\n",
    "      if y_test1[i] == y_pred1[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'logistic regression\\'s accuracy is: {100*counter/len(y_test_lr)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "lr_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca98c5c",
   "metadata": {},
   "source": [
    "## B) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd7ecdc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL_NAME = 'google/bigbird-base-trivia-itc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Y_hw4 = [y for y in df_hw4.pos_or_neg]\n",
    "X_hw4 = [x for x in df_hw4.passage]\n",
    "x_train_tr, x_testeval_tr, y_train_tr, y_testeval_tr = train_test_split(X_hw4, Y_hw4, test_size = 0.2, random_state = 1)\n",
    "x_test_tr, x_eval_tr, y_test_tr, y_eval_tr = train_test_split(x_testeval_tr, y_testeval_tr, test_size = 0.5, random_state = 1)\n",
    "train_encodings_tr = tokenizer([x for x in x_train_tr], truncation=True, padding=True)\n",
    "eval_encodings_tr = tokenizer([x for x in x_eval_tr], truncation=True, padding=True)\n",
    "test_encodings_tr = tokenizer([x for x in x_test_tr], truncation=True, padding=True)\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings=encodings\n",
    "    self.labels=labels\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_dataset_tr = TweetDataset(train_encodings_tr, y_train_tr)\n",
    "eval_dataset_tr = TweetDataset(eval_encodings_tr, y_eval_tr)\n",
    "test_dataset_tr = TweetDataset(test_encodings_tr, y_test_tr)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "c1_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "lr = 2e-5 \n",
    "weight_decay = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "accumulation_steps = 4\n",
    "num_workers = 10\n",
    "max_len = 2048 \n",
    "metric = load_metric('f1')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"Tweet-classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=lr,\n",
    "    label_smoothing_factor=0.05,\n",
    "    weight_decay=weight_decay,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=c1_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "generator = pipeline('text-classification', c1_model, tokenizer=tokenizer, config={'max_length':256})\n",
    "y_predicted_transformer = [int(y['label'].split('_')[1]) for y in generator([x for x in x_test])]\n",
    "\n",
    "def transformer_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'transformer model\\'s f1_macro score: {f1_score(y_test, y_predicted_transformer, average=\"macro\")} ')\n",
    "    results = confusion_matrix(y_test, y_predicted_transformer)\n",
    "    metrics.append(f'transformer model\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    counter = 0\n",
    "    for i in range(len(y_test)):\n",
    "      if y_test[i] == y_predicted_transformer[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'transformer model\\'s  accuracy is: {100*counter/len(y_test)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "transformer_results()\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42dfce",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### A) K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00952493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSS of our clustering with k-means algorithm is: 649.6963442107672',\n",
       " 'purity of our clustering with k-means algorithm is: 0.8014']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cluster = [x for x in df_hw4['passage']]\n",
    "vectorizer_hw4 = TfidfVectorizer(stop_words='english')\n",
    "X_cluster = vectorizer_hw4.fit_transform(X_cluster)\n",
    "\n",
    "true_k = 5\n",
    "clustering_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "clustering_model.fit(X_cluster)\n",
    "\n",
    "def get_l2_distance(vec1, vec2):\n",
    "  return float(np.dot(vec1-vec2,  (vec1-vec2).T))\n",
    "\n",
    "def calculate_RSS(kmeans_model):\n",
    "  RSS = 0\n",
    "  for point in X_cluster:\n",
    "    min_dist = get_l2_distance(kmeans_model.cluster_centers_[0], point)\n",
    "    for center in kmeans_model.cluster_centers_:\n",
    "      if get_l2_distance(point, center) < min_dist:\n",
    "        min_dist = get_l2_distance(point, center)\n",
    "    \n",
    "    RSS += min_dist/15\n",
    "  \n",
    "  return RSS\n",
    "\n",
    "def clustering_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'RSS of our clustering with k-means algorithm is: {calculate_RSS(clustering_model)}')\n",
    "\n",
    "    clusters = [[], [], [], [], []]\n",
    "    preds = [x for x in df_hw4[\"pos_or_neg\"]]\n",
    "    for i in range(1, len(preds)):\n",
    "      clusters[clustering_model.labels_[i]].append(preds[i])\n",
    "\n",
    "    true_elements = 0\n",
    "    for cluster in clusters:\n",
    "      counts = pd.Series(cluster).value_counts()\n",
    "      true_elements += max(counts.get(0), counts.get(1))\n",
    "    metrics.append(f'purity of our clustering with k-means algorithm is: {true_elements/len(preds)}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "clustering_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2583872",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "Importing data/preprocessing/using NER to get the named entities and generating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f442a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "require_cols = [0, 1, 8]\n",
    "\n",
    "df_hw5 = pd.read_excel('https://github.com/AmooHashem/my-datasets/raw/main/%40khamenei_ir_user_tweets.xlsx', usecols = require_cols, engine='openpyxl')\n",
    "\n",
    "DOC_NUMBER = len(df_hw5.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "887cb4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3167/3167 [00:21<00:00, 150.65it/s]\n",
      "100%|██████████████████████████████████| 3167/3167 [00:00<00:00, 1196483.59it/s]\n"
     ]
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "lemmatizer_hw5 = WordNetLemmatizer()\n",
    "\n",
    "def is_valid_word(text):\n",
    "  return text[0] != '@' and re.search(r\"[^\\w]\", text) is None\n",
    "\n",
    "def normalize_word(text):\n",
    "  text = re.sub(r'[\\.:!،؛؟»\\]\\)\\}«\\[\\(\\{]','', text)\n",
    "  text = remove_emojis(text)\n",
    "  return text\n",
    "\n",
    "def normalize_sent(text):\n",
    "  sent = []\n",
    "  for word in text.split():\n",
    "    if is_valid_word(word):\n",
    "      sent.append(normalize_word(word))\n",
    "  sent = ' '.join(sent)\n",
    "  sent = word_tokenize(sent)\n",
    "  sent = ' '.join(sent)\n",
    "  doc = nlp(sent)\n",
    "  sent = [(X.text, X.label_) for X in doc.ents]\n",
    "  return sent\n",
    "\n",
    "\n",
    "data_hw5 = [df_hw5.values[i][1] for i in range(len(df_hw5.values))]\n",
    "messages_hw5 = [x for x in data_hw5]\n",
    "message_entities_hw5 = []\n",
    "for message in tqdm.tqdm(messages_hw5):\n",
    "  message_entities_hw5.append(normalize_sent(str(message)))\n",
    "\n",
    "total_entities_hw5 = set()\n",
    "for entities in tqdm.tqdm(message_entities_hw5):\n",
    "  for entity in entities:\n",
    "    total_entities_hw5.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf7053c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hw5 = nx.Graph()\n",
    "G_hw5.add_nodes_from(total_entities_hw5)\n",
    "for entities in message_entities_hw5:\n",
    "  for i in range(len(entities)):\n",
    "    for j in range(i+1, len(entities)):\n",
    "      G_hw5.add_edge(entities[i], entities[j])\n",
    "\n",
    "def sort_tuples(tup):\n",
    "  lst = len(tup)\n",
    "  for i in range(0, lst):\n",
    "    for j in range(0, lst-i-1):\n",
    "      if (tup[j][1] < tup[j + 1][1]):\n",
    "        temp = tup[j]\n",
    "        tup[j]= tup[j + 1]\n",
    "        tup[j + 1]= temp\n",
    "  return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d41f12",
   "metadata": {},
   "source": [
    "## Link Analysis\n",
    "### A) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0b13819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1531/1531 [00:00<00:00, 849177.39it/s]\n"
     ]
    }
   ],
   "source": [
    "pr_hw5 = nx.pagerank(G_hw5)\n",
    "entity_prob_hw5 = []\n",
    "\n",
    "for entity in total_entities_hw5:\n",
    "  entity_prob_hw5.append((entity, pr_hw5[entity]))\n",
    "\n",
    "categories_hw5 = ['NORP', 'ORG', 'DATE', 'PERSON', 'GPE', 'LOC', 'CARDINAL']\n",
    "entity_prob_hw5 = sort_tuples(entity_prob_hw5)\n",
    "entity_category_hw5 = {}\n",
    "for entity in tqdm.tqdm(entity_prob_hw5):\n",
    "  for category in categories_hw5:\n",
    "    if entity[0][1] == category:\n",
    "      if category not in entity_category_hw5:\n",
    "        entity_category_hw5[category] = []\n",
    "      if len(entity_category_hw5[category]) < 10:\n",
    "        entity_category_hw5[category].append(entity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a9ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_results():\n",
    "    results = []\n",
    "    for category in entity_category_hw5:\n",
    "        results.append(category)\n",
    "        results.append('_______')\n",
    "        for entity in entity_category_hw5[category]:\n",
    "            results.append(f'-------{entity}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb063bc",
   "metadata": {},
   "source": [
    "### A) HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "677f0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_results(num_outputs=10):\n",
    "    results = []\n",
    "    h_hw5, a_hw5 = nx.hits(G_hw5)\n",
    "    hubs_hw5 = []\n",
    "    for hub in h_hw5:\n",
    "      hubs_hw5.append((hub, h_hw5[hub]))\n",
    "\n",
    "    hubs_hw5 = sort_tuples(hubs_hw5)\n",
    "\n",
    "    authorities_hw5 = []\n",
    "    for authority in a_hw5:\n",
    "      authorities_hw5.append((authority, a_hw5[authority]))\n",
    "\n",
    "    authorities_hw5 = sort_tuples(authorities_hw5)\n",
    "\n",
    "    results .append(f'Top {num_outputs} hubs: ')\n",
    "    results .append('_____')\n",
    "    for hub in hubs_hw5[:num_outputs]:\n",
    "        results.append('(' + hub[0][0] + ', ' + hub[0][1] + f' ) with score: {hub[1]}')\n",
    "        \n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results .append(f'Top {num_outputs} authorities: ')\n",
    "    results .append('_____')\n",
    "    for authority in authorities_hw5[:num_outputs]:\n",
    "        results.append('(' + authority[0][0] + ', ' + authority[0][1] + f' ) with score: {authority[1]}')\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f16a6f",
   "metadata": {},
   "source": [
    "# Setting up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "980f02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, query_type, queryExp):\n",
    "    if query_type == 'classification':\n",
    "        return (lr_results(), []) #+ transformer_results()\n",
    "    elif query_type == 'clustering':\n",
    "        return (clustering_results(), [])\n",
    "    elif query_type == 'hits':\n",
    "        return (hits_results(10), [])\n",
    "    elif query_type == 'page_rank':\n",
    "        return (page_rank_results(), [])\n",
    "    elif query_type == 'boolean' or query_type == 'tf_idf' or query_type == 'transformer' or query_type == 'fastText':\n",
    "        return get_hw3_results(query, query_type, queryExp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78bef9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started http://localhost:8080\n",
      "{'query': 'gjhhjv. sd s', 'queryExp': True, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 61548)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 427, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 415, in handle_one_request\n",
      "    method()\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1460256267.py\", line 29, in do_POST\n",
      "    response, expanded_queries = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/3502920012.py\", line 11, in generate_response\n",
      "    return get_hw3_results(query, query_type, queryExp)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/86339139.py\", line 3, in get_hw3_results\n",
      "    query_results = search_hw3(query, query_type)[0]\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1071828786.py\", line 2, in search_hw3\n",
      "    return search_mudoles[modul_type](query, output_size)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/828152368.py\", line 4, in search_with_tf_idf\n",
      "    input_tfidf = vectorizer.transform([user_query]).toarray()[0]\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2101, in transform\n",
      "    X = super().transform(raw_documents)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1379, in transform\n",
      "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "AttributeError: 'list' object has no attribute 'lower'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'gjhhjv. sd s', 'queryExp': True, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 61550)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 427, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 415, in handle_one_request\n",
      "    method()\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1460256267.py\", line 29, in do_POST\n",
      "    response, expanded_queries = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/3502920012.py\", line 11, in generate_response\n",
      "    return get_hw3_results(query, query_type, queryExp)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/86339139.py\", line 3, in get_hw3_results\n",
      "    query_results = search_hw3(query, query_type)[0]\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1071828786.py\", line 2, in search_hw3\n",
      "    return search_mudoles[modul_type](query, output_size)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/828152368.py\", line 4, in search_with_tf_idf\n",
      "    input_tfidf = vectorizer.transform([user_query]).toarray()[0]\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2101, in transform\n",
      "    X = super().transform(raw_documents)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1379, in transform\n",
      "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "AttributeError: 'list' object has no attribute 'lower'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'gjhhjv. sd s', 'queryExp': True, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 61552)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 427, in handle\n",
      "    self.handle_one_request()\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/http/server.py\", line 415, in handle_one_request\n",
      "    method()\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1460256267.py\", line 29, in do_POST\n",
      "    response, expanded_queries = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/3502920012.py\", line 11, in generate_response\n",
      "    return get_hw3_results(query, query_type, queryExp)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/86339139.py\", line 3, in get_hw3_results\n",
      "    query_results = search_hw3(query, query_type)[0]\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/1071828786.py\", line 2, in search_hw3\n",
      "    return search_mudoles[modul_type](query, output_size)\n",
      "  File \"/var/folders/3l/_y3czlc14gbbggq71kqclws00000gn/T/ipykernel_10340/828152368.py\", line 4, in search_with_tf_idf\n",
      "    input_tfidf = vectorizer.transform([user_query]).toarray()[0]\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 2101, in transform\n",
      "    X = super().transform(raw_documents)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1379, in transform\n",
      "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 113, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"/Users/imanalipour/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\", line 71, in _preprocess\n",
      "    doc = doc.lower()\n",
      "AttributeError: 'list' object has no attribute 'lower'\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'classification', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Jul/2022 16:30:33] \"POST /classification/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Jul/2022 16:30:36] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'page_rank', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Jul/2022 16:30:38] \"POST /page_rank/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'clustering', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [26/Jul/2022 16:30:48] \"POST /clustering/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server stopped.\n"
     ]
    }
   ],
   "source": [
    "hostName = \"localhost\"\n",
    "serverPort = 8080\n",
    "\n",
    "class MyServer(BaseHTTPRequestHandler):\n",
    "    def end_headers (self):\n",
    "        self.send_header('Access-Control-Allow-Origin', '*')\n",
    "        SimpleHTTPRequestHandler.end_headers(self)\n",
    "        \n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-type\", \"text/html\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(bytes(\"<html><head><title>https://pythonbasics.org</title></head>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>Request: %s</p>\" % self.path, \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>This is an example web server.</p>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
    "        \n",
    "    def do_POST(self):\n",
    "        # 1. How long was the message?\n",
    "        length = int(self.headers.get('Content-length', 0))\n",
    "\n",
    "        # 2. Read the correct amount of data from the request.\n",
    "        data = self.rfile.read(length).decode()\n",
    "\n",
    "        # 3. Extract the \"message\" field from the request data.\n",
    "        message = json.loads(data)['query']\n",
    "        print(json.loads(data))\n",
    "        response, expanded_queries = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
    "        reply = {'answers': response, 'expandedQueries': expanded_queries}\n",
    "        \n",
    "        # Send the \"message\" field back as the response.        \n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/plain; charset=utf-8')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(reply).encode())\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    webServer = HTTPServer((hostName, serverPort), MyServer)\n",
    "    print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
    "\n",
    "    try:\n",
    "        webServer.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    webServer.server_close()\n",
    "    print(\"Server stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
