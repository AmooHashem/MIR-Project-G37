{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/erfan/.local/lib/python3.8/site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /home/erfan/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: sklearn in /home/erfan/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: nltk in /home/erfan/.local/lib/python3.8/site-packages (3.3)\n",
      "Collecting pyg-nightly\n",
      "  Downloading pyg-nightly-2.0.5.dev20220726.tar.gz (455 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.5/455.5 kB\u001b[0m \u001b[31m704.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m695.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m477.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:40\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/erfan/.local/lib/python3.8/site-packages (0.1.96)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m925.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m984.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/erfan/.local/lib/python3.8/site-packages (1.4.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m856.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/erfan/.local/lib/python3.8/site-packages (1.22.3)\n",
      "Collecting folium\n",
      "  Downloading folium-0.12.1.post1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m868.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting streamlit\n",
      "  Downloading streamlit-1.11.0-py2.py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m740.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pickle5 in /home/erfan/.local/lib/python3.8/site-packages (0.0.11)\n",
      "Requirement already satisfied: hazm in /home/erfan/.local/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: tqdm in /home/erfan/.local/lib/python3.8/site-packages (4.64.0)\n",
      "Collecting networkx==2.6.3\n",
      "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m949.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m748.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement en_core_web_sm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for en_core_web_sm\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install datasets sklearn nltk pyg-nightly tabulate torch sklearn sentencepiece transformers sentence-transformers pandas faiss-cpu numpy folium streamlit datasets pickle5 sklearn hazm nltk tqdm networkx==2.6.3 spacy en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/erfan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[nltk_data] Downloading package punkt to /home/erfan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import socket, time\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer, SimpleHTTPRequestHandler, test\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, BigBirdModel, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download(\"popular\")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "# import faiss\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n",
    "from tabulate import tabulate\n",
    "from textblob import TextBlob\n",
    "from textblob.en import Spelling        \n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw3 = pd.read_csv('../../../sentiment140.csv') ###############333\n",
    "df_hw3.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw3['pos_or_neg'].replace({4: 1}, inplace=True)\n",
    "DOC_NUMBER = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 100\n",
    "df_hw3 = df_hw3.sample(n=max_dataset_items)\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def normalize_sent(text):\n",
    "    text = normalize_text(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda x: normalize_text(x))\n",
    "# df_hw3['passage'] = df_hw3['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"Text\"] = df_hw3['passage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boolean_df():\n",
    "  all_words = list(itertools.chain(*[[t for t in message.split()] for message in df[\"Text\"]]))\n",
    "\n",
    "  empty_dataframe_input = {}\n",
    "  number_of_tweets = len(df[\"Text\"])\n",
    "  for word in all_words:\n",
    "    empty_dataframe_input[word] = [0 for i in range(0, number_of_tweets)]\n",
    "\n",
    "  boolean_df = pd.DataFrame(empty_dataframe_input)\n",
    "  for i in range(len(df[\"Text\"])):\n",
    "    words = word_tokenize(df[\"Text\"].iloc[i])\n",
    "    for word in words:\n",
    "      boolean_df.at[i, word] = 1\n",
    "  return boolean_df\n",
    "boolean_df = create_boolean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_boolean_embeddings(user_query, output_size = 10):\n",
    "    sent = normalize_sent(user_query)\n",
    "\n",
    "    out = {}\n",
    "    for i in range(len(df[\"Text\"])):\n",
    "        out[i] = 0\n",
    "    out = pd.Series(data=out)\n",
    "    for word in sent:\n",
    "        if word in boolean_df.columns:\n",
    "            out = out + boolean_df[word]\n",
    "    out = out.sort_values(ascending=False)\n",
    "\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df[\"Text\"].iloc[out.index[i]]])\n",
    "\n",
    "    return [result, 'boolean']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_df = vectorizer.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_tf_idf(user_query, output_size = 10):\n",
    "    user_query = normalize_sent(user_query)\n",
    "\n",
    "    input_tfidf = vectorizer.transform([user_query]).toarray()[0]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(DOC_NUMBER):\n",
    "        similarity = tfidf_df[i].dot(input_tfidf)[0]\n",
    "        scores.append((i, similarity))\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df['Text'].iloc[scores[i][0]]])\n",
    "    return [result, 'tf-idf']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, model, index, num_results=10):\n",
    "    vector = model.encode(list(query))\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
    "    return D, I\n",
    "\n",
    "def id2details(df, I, column):\n",
    "    return [list(df[df.id == idx][column]) for idx in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/distilbert-base-nli-stsb-mean-tokens (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3d1c047310>: Failed to establish a new connection: [Errno 110] Connection timed out'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f3d1c047310>: Failed to establish a new connection: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/distilbert-base-nli-stsb-mean-tokens (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3d1c047310>: Failed to establish a new connection: [Errno 110] Connection timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-c0da936b9528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-nli-stsb-mean-tokens'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'modules.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;31m# Download from hub with caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     snapshot_download(model_name_or_path,\n\u001b[0m\u001b[1;32m     88\u001b[0m                                         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                                         \u001b[0mlibrary_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sentence-transformers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mmodel_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     storage_folder = os.path.join(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             args_msg = [\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout, securityStatus)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Bearer {token}\"\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0mstatus_query_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"securityStatus\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msecurityStatus\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         r = requests.get(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus_query_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/sentence-transformers/distilbert-base-nli-stsb-mean-tokens (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3d1c047310>: Failed to establish a new connection: [Errno 110] Connection timed out'))"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "embeddings = model.encode(df.Text.to_list(), show_progress_bar=True)\n",
    "print(f'Shape of the vectorised abstract: {embeddings[0].shape}')\n",
    "\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index = faiss.IndexIDMap(index)\n",
    "index.add_with_ids(embeddings, df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")\n",
    "df.iloc[400, 1]\n",
    "D, I = index.search(np.array([embeddings[1]]), k=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "print(id2details(df, I, 'Text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_transformers(user_query, output_size = 10):\n",
    "  user_query = normalize_sent(user_query)\n",
    "  D, I = vector_search([user_query], model, index, num_results=output_size)\n",
    "  #print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "  return[id2details(df, I, 'Text'), 'transformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x.split() for x in df['Text']]\n",
    "\n",
    "ftmodel = FastText(sentences=data, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_fastText_embeddings(user_query, output_size = 10):\n",
    "    user_query = normalize_sent(user_query)\n",
    "    user_query = user_query.split()\n",
    "    scores = []\n",
    "    for document in df[\"Text\"]:\n",
    "        if document != '':\n",
    "            similarity = ftmodel.wv.n_similarity(user_query, document.split())\n",
    "            scores.append((document, similarity))\n",
    "\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([scores[i][0]])\n",
    "\n",
    "    return [result, 'fastText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_mudoles = {\n",
    "    'boolean': search_with_boolean_embeddings,\n",
    "    'tf_idf': search_with_tf_idf, \n",
    "#     'transformer': search_with_transformers, \n",
    "    'fastText': search_with_fastText_embeddings\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hw3(query, modul_type):\n",
    "    return search_mudoles[modul_type]('hello', output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model():\n",
    "    # Create a placeholder for model\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for sentence in df['Text'] :\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split(' ')\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model[(w1, w2)][w3] += 1\n",
    "        \n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model2[w1][w2] += 1\n",
    "            \n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1_w2 in model:\n",
    "        total_count = float(sum(model[w1_w2].values()))\n",
    "        for w3 in model[w1_w2]:\n",
    "            model[w1_w2][w3] /= total_count\n",
    "    \n",
    "    for w1 in model2:\n",
    "        total_count = float(sum(model2[w1].values()))\n",
    "        for w2 in model2[w1]:\n",
    "            model2[w1][w2] /= total_count\n",
    "    \n",
    "    return model, model2\n",
    "\n",
    "def create_spell_checker_input():\n",
    "    oneString = ''\n",
    "    for sentence in df['Text'] :\n",
    "        textToLower = sentence.lower()\n",
    "        words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
    "        oneString += \" \".join(words)  \n",
    "    pathToFile = \"textblobtrain.txt\"                              # The path we want to store our stats file at\n",
    "    spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
    "    spelling.train(oneString, pathToFile)                 # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction_defualt(text):\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    return textCorrected\n",
    "\n",
    "def spell_correction(text):\n",
    "    pathToFile = \"textblobtrain.txt\" \n",
    "    spelling = Spelling(path = pathToFile)\n",
    "    words = text.split()\n",
    "    corrected = \" \"\n",
    "    for i in words :\n",
    "        corrected = corrected +\" \"+ spelling.suggest(i)[0][0] # Spell checking word by word\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model3, language_model2 = create_language_model()\n",
    "create_spell_checker_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(w):\n",
    "    return w.lower() if w is not None else None\n",
    "\n",
    "def get_next_word_list(model, w1, w2):\n",
    "    w1 = to_lower(w1)\n",
    "    w2 = to_lower(w2)\n",
    "    return sorted(dict(model[w1, w2]).items(), key=lambda x:-1*x[1])\n",
    "\n",
    "def expand_with_language_model(query):\n",
    "    query_list = [str(query)]\n",
    "    sentence = query\n",
    "    sentence = sentence.split(' ')\n",
    "    indx = 0\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        sent = sentence.copy()[:indx+2]\n",
    "        a, b = w1, w2\n",
    "        for i in range(len(sentence) - indx):\n",
    "            c_list = get_next_word_list(language_model3, a, b)\n",
    "\n",
    "            try:\n",
    "                if c_list[0][0] is None:\n",
    "                    break\n",
    "                sent.append(c_list[0][0])\n",
    "                sent2 = sent.copy()\n",
    "                sent2.extend(sentence[len(sent):])\n",
    "                query_list.append(' '.join(sent2))\n",
    "                a, b = b, c_list[0][0]\n",
    "            except:\n",
    "                break\n",
    "        indx += 1\n",
    "\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(model, sentence):\n",
    "    epsilone = 1e-10\n",
    "    out = 0\n",
    "    indx = 0\n",
    "    sentence = sentence.split(' ')\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        w1, w2 = to_lower(w1), to_lower(w2)\n",
    "        p = dict(model[w1])[w2] if w2 in model[w1].keys() else 0\n",
    "        p = math.log(p + epsilone)\n",
    "        out += p\n",
    "        indx += 1\n",
    "    return out/indx\n",
    "\n",
    "def expand_query(query, max_out = 7):\n",
    "    query_list = []\n",
    "\n",
    "    query_list.extend(expand_with_language_model(query))\n",
    "    query_list.extend(expand_with_language_model(spell_correction(query)))\n",
    "    query_list.extend(expand_with_language_model(spell_correction_defualt(query)))\n",
    "    l = []\n",
    "    for q in query_list:\n",
    "        l.append((get_p_value(language_model2, q), q.strip()))\n",
    "    l = sorted(l, key=lambda x:-1*x[0])[:max_out]\n",
    "    return [q for _, q in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['111 am gonna have a hard time waking up for school'],\n",
       "  ['bed gotta me up in a few hour shame the weather is gonna b bad'],\n",
       "  ['our sweet little man just fell asleep while waiting up for daddy and big brother to get home poor baby he missed them so much today'],\n",
       "  ['seems im not the only one unable to get to sleep urgh it just too warm'],\n",
       "  ['yeah cross your finger'],\n",
       "  ['oh cool wish i could go to london'],\n",
       "  ['those were just the one i know that are on twitter oh amp twitter is on it too article is almost done just a few tweak'],\n",
       "  ['my friend had serious face on i assumed they were doing their work i looked at their laptop amp they were fbing i feel left out'],\n",
       "  ['six place and no purse really struck me a must have it probably just my mood'],\n",
       "  ['beef w brocoli today not the same a last time i had it it actually kinda gross']],\n",
       " 'fastText']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"hapened to me wih an\"\n",
    "expand_query(query, 10)\n",
    "search_hw3(query, 'fastText')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4\n",
    "## HW4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw4 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw4.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw4['pos_or_neg'].replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw4 = df_hw4.sample(n=max_dataset_items)\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: normalize_text(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "### A) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"logistic regression's f1_macro score: 0.502116006970376\",\n",
       " \"logistic regression's f1_micro data score: 0.816\",\n",
       " \"logistic regression's confusion matrix:\",\n",
       " '[805   3]',\n",
       " '[181  11]',\n",
       " \"logistic regression's accuracy is: 81.6%\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_hw4 = TfidfVectorizer(ngram_range=(1,2), stop_words = None, norm='l2')\n",
    "x_tfidf_hw4 = vectorizer_hw4.fit_transform(df_hw4.passage)\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf_hw4, df_hw4.pos_or_neg, test_size = 0.1, random_state = 1)\n",
    "regressor_hw4 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(x_train_lr, y_train_lr)\n",
    "\n",
    "def lr_results():\n",
    "    metrics = []\n",
    "    \n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='macro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_macro score: {score}')\n",
    "\n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='micro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_micro data score: {score}')\n",
    "\n",
    "    results = confusion_matrix(y_test_lr, y_predicted)\n",
    "    metrics.append(f'logistic regression\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    \n",
    "\n",
    "    y_test1 = [y for y in y_test_lr]\n",
    "    y_pred1 = [y for y in y_predicted]\n",
    "    counter = 0\n",
    "    for i in range(len(y_test_lr)):\n",
    "      if y_test1[i] == y_pred1[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'logistic regression\\'s accuracy is: {100*counter/len(y_test_lr)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "lr_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL_NAME = 'google/bigbird-base-trivia-itc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Y_hw4 = [y for y in df_hw4.pos_or_neg]\n",
    "X_hw4 = [x for x in df_hw4.passage]\n",
    "x_train_tr, x_testeval_tr, y_train_tr, y_testeval_tr = train_test_split(X_hw4, Y_hw4, test_size = 0.2, random_state = 1)\n",
    "x_test_tr, x_eval_tr, y_test_tr, y_eval_tr = train_test_split(x_testeval_tr, y_testeval_tr, test_size = 0.5, random_state = 1)\n",
    "train_encodings_tr = tokenizer([x for x in x_train_tr], truncation=True, padding=True)\n",
    "eval_encodings_tr = tokenizer([x for x in x_eval_tr], truncation=True, padding=True)\n",
    "test_encodings_tr = tokenizer([x for x in x_test_tr], truncation=True, padding=True)\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings=encodings\n",
    "    self.labels=labels\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_dataset_tr = TweetDataset(train_encodings_tr, y_train_tr)\n",
    "eval_dataset_tr = TweetDataset(eval_encodings_tr, y_eval_tr)\n",
    "test_dataset_tr = TweetDataset(test_encodings_tr, y_test_tr)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "c1_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "lr = 2e-5 \n",
    "weight_decay = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "accumulation_steps = 4\n",
    "num_workers = 10\n",
    "max_len = 2048 \n",
    "metric = load_metric('f1')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"Tweet-classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=lr,\n",
    "    label_smoothing_factor=0.05,\n",
    "    weight_decay=weight_decay,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=c1_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "generator = pipeline('text-classification', c1_model, tokenizer=tokenizer, config={'max_length':256})\n",
    "y_predicted_transformer = [int(y['label'].split('_')[1]) for y in generator([x for x in x_test])]\n",
    "\n",
    "def transformer_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'transformer model\\'s f1_macro score: {f1_score(y_test, y_predicted_transformer, average=\"macro\")} ')\n",
    "    results = confusion_matrix(y_test, y_predicted_transformer)\n",
    "    metrics.append(f'transformer model\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    counter = 0\n",
    "    for i in range(len(y_test)):\n",
    "      if y_test[i] == y_predicted_transformer[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'transformer model\\'s  accuracy is: {100*counter/len(y_test)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "transformer_results()\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### A) K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSS of our clustering with k-means algorithm is: 650.8825714095935',\n",
       " 'purity of our clustering with k-means algorithm is: 0.805']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cluster = [x for x in df_hw4['passage']]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_cluster = vectorizer.fit_transform(X_cluster)\n",
    "\n",
    "true_k = 5\n",
    "clustering_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "clustering_model.fit(X_cluster)\n",
    "\n",
    "def get_l2_distance(vec1, vec2):\n",
    "  return float(np.dot(vec1-vec2,  (vec1-vec2).T))\n",
    "\n",
    "def calculate_RSS(kmeans_model):\n",
    "  RSS = 0\n",
    "  for point in X_cluster:\n",
    "    min_dist = get_l2_distance(kmeans_model.cluster_centers_[0], point)\n",
    "    for center in kmeans_model.cluster_centers_:\n",
    "      if get_l2_distance(point, center) < min_dist:\n",
    "        min_dist = get_l2_distance(point, center)\n",
    "    \n",
    "    RSS += min_dist/15\n",
    "  \n",
    "  return RSS\n",
    "\n",
    "def clustering_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'RSS of our clustering with k-means algorithm is: {calculate_RSS(clustering_model)}')\n",
    "\n",
    "    clusters = [[], [], [], [], []]\n",
    "    preds = [x for x in df_hw4[\"pos_or_neg\"]]\n",
    "    for i in range(1, len(preds)):\n",
    "      clusters[clustering_model.labels_[i]].append(preds[i])\n",
    "\n",
    "    true_elements = 0\n",
    "    for cluster in clusters:\n",
    "      counts = pd.Series(cluster).value_counts()\n",
    "      true_elements += max(counts.get(0), counts.get(1))\n",
    "    metrics.append(f'purity of our clustering with k-means algorithm is: {true_elements/len(preds)}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "clustering_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "Importing data/preprocessing/using NER to get the named entities and generating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "require_cols = [0, 1, 8]\n",
    "\n",
    "df_hw5 = pd.read_excel('https://github.com/AmooHashem/my-datasets/raw/main/%40khamenei_ir_user_tweets.xlsx', usecols = require_cols, engine='openpyxl')\n",
    "\n",
    "DOC_NUMBER = len(df_hw5.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3167/3167 [00:19<00:00, 160.09it/s]\n",
      "100%|██████████████████████████████████| 3167/3167 [00:00<00:00, 1318971.38it/s]\n"
     ]
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "lemmatizer_hw5 = WordNetLemmatizer()\n",
    "\n",
    "def is_valid_word(text):\n",
    "  return text[0] != '@' and re.search(r\"[^\\w]\", text) is None\n",
    "\n",
    "def normalize_word(text):\n",
    "  text = re.sub(r'[\\.:!،؛؟»\\]\\)\\}«\\[\\(\\{]','', text)\n",
    "  text = remove_emojis(text)\n",
    "  return text\n",
    "\n",
    "def normalize_sent(text):\n",
    "  sent = []\n",
    "  for word in text.split():\n",
    "    if is_valid_word(word):\n",
    "      sent.append(normalize_word(word))\n",
    "  sent = ' '.join(sent)\n",
    "  sent = word_tokenize(sent)\n",
    "  sent = ' '.join(sent)\n",
    "  doc = nlp(sent)\n",
    "  sent = [(X.text, X.label_) for X in doc.ents]\n",
    "  return sent\n",
    "\n",
    "\n",
    "data_hw5 = [df_hw5.values[i][1] for i in range(len(df_hw5.values))]\n",
    "messages_hw5 = [x for x in data_hw5]\n",
    "message_entities_hw5 = []\n",
    "for message in tqdm.tqdm(messages_hw5):\n",
    "  message_entities_hw5.append(normalize_sent(str(message)))\n",
    "\n",
    "total_entities_hw5 = set()\n",
    "for entities in tqdm.tqdm(message_entities_hw5):\n",
    "  for entity in entities:\n",
    "    total_entities_hw5.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hw5 = nx.Graph()\n",
    "G_hw5.add_nodes_from(total_entities_hw5)\n",
    "for entities in message_entities_hw5:\n",
    "  for i in range(len(entities)):\n",
    "    for j in range(i+1, len(entities)):\n",
    "      G_hw5.add_edge(entities[i], entities[j])\n",
    "\n",
    "def sort_tuples(tup):\n",
    "  lst = len(tup)\n",
    "  for i in range(0, lst):\n",
    "    for j in range(0, lst-i-1):\n",
    "      if (tup[j][1] < tup[j + 1][1]):\n",
    "        temp = tup[j]\n",
    "        tup[j]= tup[j + 1]\n",
    "        tup[j + 1]= temp\n",
    "  return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Analysis\n",
    "### A) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1549/1549 [00:00<00:00, 932535.80it/s]\n"
     ]
    }
   ],
   "source": [
    "pr_hw5 = nx.pagerank(G_hw5)\n",
    "entity_prob_hw5 = []\n",
    "\n",
    "for entity in total_entities_hw5:\n",
    "  entity_prob_hw5.append((entity, pr_hw5[entity]))\n",
    "\n",
    "categories_hw5 = ['NORP', 'ORG', 'DATE', 'PERSON', 'GPE', 'LOC', 'CARDINAL']\n",
    "entity_prob_hw5 = sort_tuples(entity_prob_hw5)\n",
    "entity_category_hw5 = {}\n",
    "for entity in tqdm.tqdm(entity_prob_hw5):\n",
    "  for category in categories_hw5:\n",
    "    if entity[0][1] == category:\n",
    "      if category not in entity_category_hw5:\n",
    "        entity_category_hw5[category] = []\n",
    "      if len(entity_category_hw5[category]) < 10:\n",
    "        entity_category_hw5[category].append(entity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_results():\n",
    "    results = []\n",
    "    for category in entity_category_hw5:\n",
    "        results.append(category)\n",
    "        results.append('_______')\n",
    "        for entity in entity_category_hw5[category]:\n",
    "            results.append(f'-------{entity}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_results(num_outputs=10):\n",
    "    results = []\n",
    "    h_hw5, a_hw5 = nx.hits(G_hw5)\n",
    "    hubs_hw5 = []\n",
    "    for hub in h_hw5:\n",
    "      hubs_hw5.append((hub, h_hw5[hub]))\n",
    "\n",
    "    hubs_hw5 = sort_tuples(hubs_hw5)\n",
    "\n",
    "    authorities_hw5 = []\n",
    "    for authority in a_hw5:\n",
    "      authorities_hw5.append((authority, a_hw5[authority]))\n",
    "\n",
    "    authorities_hw5 = sort_tuples(authorities_hw5)\n",
    "\n",
    "    results .append(f'Top {num_outputs} hubs: ')\n",
    "    results .append('_____')\n",
    "    for hub in hubs_hw5[:num_outputs]:\n",
    "        results.append('(' + hub[0][0] + ', ' + hub[0][1] + f' ) with score: {hub[1]}')\n",
    "        \n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results .append(f'Top {num_outputs} authorities: ')\n",
    "    results .append('_____')\n",
    "    for authority in authorities_hw5[:num_outputs]:\n",
    "        results.append('(' + authority[0][0] + ', ' + authority[0][1] + f' ) with score: {authority[1]}')\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, query_type, queryExp):\n",
    "    if query_type == 'classification':\n",
    "        return lr_results() #+ transformer_results()\n",
    "    elif query_type == 'clustering':\n",
    "        return clustering_results()\n",
    "    elif query_type == 'hits':\n",
    "        return hits_results(10)\n",
    "    elif query_type == 'page_rank':\n",
    "        return page_rank_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostName = \"localhost\"\n",
    "serverPort = 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started http://localhost:8080\n",
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:22:32] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'page_rank', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:22:44] \"POST /page_rank/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'classification', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:23:51] \"POST /classification/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'classification', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:24:02] \"POST /classification/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'clustering', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:24:25] \"POST /clustering/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'clustering', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:24:36] \"POST /clustering/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'infjidnfisffsdf', 'queryExp': True, 'type': 'transformer', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Jul/2022 17:25:21] \"POST /transformer/ HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "class MyServer(BaseHTTPRequestHandler):\n",
    "    def end_headers (self):\n",
    "        self.send_header('Access-Control-Allow-Origin', '*')\n",
    "        SimpleHTTPRequestHandler.end_headers(self)\n",
    "        \n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-type\", \"text/html\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(bytes(\"<html><head><title>https://pythonbasics.org</title></head>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>Request: %s</p>\" % self.path, \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>This is an example web server.</p>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
    "        \n",
    "    def do_POST(self):\n",
    "        # 1. How long was the message?\n",
    "        length = int(self.headers.get('Content-length', 0))\n",
    "\n",
    "        # 2. Read the correct amount of data from the request.\n",
    "        data = self.rfile.read(length).decode()\n",
    "\n",
    "        # 3. Extract the \"message\" field from the request data.\n",
    "        message = json.loads(data)['query']\n",
    "        print(json.loads(data))\n",
    "        response = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
    "        reply = {'answers': response, 'expanded_query': 'I\\'m an exmanded query and I\\'m very very happy panda! 🐼  :D'}\n",
    "        \n",
    "        # Send the \"message\" field back as the response.        \n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/plain; charset=utf-8')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(reply).encode())\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    webServer = HTTPServer((hostName, serverPort), MyServer)\n",
    "    print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
    "\n",
    "    try:\n",
    "        webServer.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    webServer.server_close()\n",
    "    print(\"Server stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
