{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37259628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyg-nightly tabulate torch sklearn sentencepiece transformers sentence-transformers pandas faiss-cpu numpy folium streamlit datasets pickle5 sklearn hazm nltk tqdm networkx==2.6.3 spacy en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb082e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import socket, time\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer, SimpleHTTPRequestHandler, test\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, BigBirdModel, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download(\"popular\")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "import faiss\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705c891",
   "metadata": {},
   "source": [
    "# HW4\n",
    "## HW4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3920fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw4 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw4.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw4['pos_or_neg'].replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b58414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw4 = df_hw4.sample(n=max_dataset_items)\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: normalize_text(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccedde",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "### A) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cb09d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"logistic regression's f1_macro score: 0.502292471042471\",\n",
       " \"logistic regression's f1_micro data score: 0.802\",\n",
       " \"logistic regression's confusion matrix:\",\n",
       " '[789   1]',\n",
       " '[197  13]',\n",
       " \"logistic regression's accuracy is: 80.2%\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_hw4 = TfidfVectorizer(ngram_range=(1,2), stop_words = None, norm='l2')\n",
    "x_tfidf_hw4 = vectorizer_hw4.fit_transform(df_hw4.passage)\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf_hw4, df_hw4.pos_or_neg, test_size = 0.1, random_state = 1)\n",
    "regressor_hw4 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(x_train_lr, y_train_lr)\n",
    "\n",
    "def lr_results():\n",
    "    metrics = []\n",
    "    \n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='macro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_macro score: {score}')\n",
    "\n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='micro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_micro data score: {score}')\n",
    "\n",
    "    results = confusion_matrix(y_test_lr, y_predicted)\n",
    "    metrics.append(f'logistic regression\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    \n",
    "\n",
    "    y_test1 = [y for y in y_test_lr]\n",
    "    y_pred1 = [y for y in y_predicted]\n",
    "    counter = 0\n",
    "    for i in range(len(y_test_lr)):\n",
    "      if y_test1[i] == y_pred1[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'logistic regression\\'s accuracy is: {100*counter/len(y_test_lr)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "lr_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7cde3",
   "metadata": {},
   "source": [
    "## B) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64dfb73a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL_NAME = 'google/bigbird-base-trivia-itc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Y_hw4 = [y for y in df_hw4.pos_or_neg]\n",
    "X_hw4 = [x for x in df_hw4.passage]\n",
    "x_train_tr, x_testeval_tr, y_train_tr, y_testeval_tr = train_test_split(X_hw4, Y_hw4, test_size = 0.2, random_state = 1)\n",
    "x_test_tr, x_eval_tr, y_test_tr, y_eval_tr = train_test_split(x_testeval_tr, y_testeval_tr, test_size = 0.5, random_state = 1)\n",
    "train_encodings_tr = tokenizer([x for x in x_train_tr], truncation=True, padding=True)\n",
    "eval_encodings_tr = tokenizer([x for x in x_eval_tr], truncation=True, padding=True)\n",
    "test_encodings_tr = tokenizer([x for x in x_test_tr], truncation=True, padding=True)\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings=encodings\n",
    "    self.labels=labels\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_dataset_tr = TweetDataset(train_encodings_tr, y_train_tr)\n",
    "eval_dataset_tr = TweetDataset(eval_encodings_tr, y_eval_tr)\n",
    "test_dataset_tr = TweetDataset(test_encodings_tr, y_test_tr)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "c1_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "lr = 2e-5 \n",
    "weight_decay = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "accumulation_steps = 4\n",
    "num_workers = 10\n",
    "max_len = 2048 \n",
    "metric = load_metric('f1')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"Tweet-classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=lr,\n",
    "    label_smoothing_factor=0.05,\n",
    "    weight_decay=weight_decay,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=c1_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "generator = pipeline('text-classification', c1_model, tokenizer=tokenizer, config={'max_length':256})\n",
    "y_predicted_transformer = [int(y['label'].split('_')[1]) for y in generator([x for x in x_test])]\n",
    "\n",
    "def transformer_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'transformer model\\'s f1_macro score: {f1_score(y_test, y_predicted_transformer, average=\"macro\")} ')\n",
    "    results = confusion_matrix(y_test, y_predicted_transformer)\n",
    "    metrics.append(f'transformer model\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    counter = 0\n",
    "    for i in range(len(y_test)):\n",
    "      if y_test[i] == y_predicted_transformer[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'transformer model\\'s  accuracy is: {100*counter/len(y_test)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "transformer_results()\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23899332",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### A) K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1807de65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSS of our clustering with k-means algorithm is: 653.6047237872614',\n",
       " 'purity of our clustering with k-means algorithm is: 0.8008']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cluster = [x for x in df_hw4['passage']]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_cluster = vectorizer.fit_transform(X_cluster)\n",
    "\n",
    "true_k = 5\n",
    "clustering_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "clustering_model.fit(X_cluster)\n",
    "\n",
    "def get_l2_distance(vec1, vec2):\n",
    "  return float(np.dot(vec1-vec2,  (vec1-vec2).T))\n",
    "\n",
    "def calculate_RSS(kmeans_model):\n",
    "  RSS = 0\n",
    "  for point in X_cluster:\n",
    "    min_dist = get_l2_distance(kmeans_model.cluster_centers_[0], point)\n",
    "    for center in kmeans_model.cluster_centers_:\n",
    "      if get_l2_distance(point, center) < min_dist:\n",
    "        min_dist = get_l2_distance(point, center)\n",
    "    \n",
    "    RSS += min_dist/15\n",
    "  \n",
    "  return RSS\n",
    "\n",
    "def clustering_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'RSS of our clustering with k-means algorithm is: {calculate_RSS(clustering_model)}')\n",
    "\n",
    "    clusters = [[], [], [], [], []]\n",
    "    preds = [x for x in df_hw4[\"pos_or_neg\"]]\n",
    "    for i in range(1, len(preds)):\n",
    "      clusters[clustering_model.labels_[i]].append(preds[i])\n",
    "\n",
    "    true_elements = 0\n",
    "    for cluster in clusters:\n",
    "      counts = pd.Series(cluster).value_counts()\n",
    "      true_elements += max(counts.get(0), counts.get(1))\n",
    "    metrics.append(f'purity of our clustering with k-means algorithm is: {true_elements/len(preds)}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "clustering_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e29821",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "Importing data/preprocessing/using NER to get the named entities and generating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a0eb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "require_cols = [0, 1, 8]\n",
    "\n",
    "df_hw5 = pd.read_excel('https://github.com/AmooHashem/my-datasets/raw/main/%40khamenei_ir_user_tweets.xlsx', usecols = require_cols, engine='openpyxl')\n",
    "\n",
    "DOC_NUMBER = len(df_hw5.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a041a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3167/3167 [00:18<00:00, 171.22it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3167/3167 [00:00<00:00, 929556.39it/s]\n"
     ]
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "lemmatizer_hw5 = WordNetLemmatizer()\n",
    "\n",
    "def is_valid_word(text):\n",
    "  return text[0] != '@' and re.search(r\"[^\\w]\", text) is None\n",
    "\n",
    "def normalize_word(text):\n",
    "  text = re.sub(r'[\\.:!ÿåÿõÿü¬ª\\]\\)\\}¬´\\[\\(\\{]','', text)\n",
    "  text = remove_emojis(text)\n",
    "  return text\n",
    "\n",
    "def normalize_sent(text):\n",
    "  sent = []\n",
    "  for word in text.split():\n",
    "    if is_valid_word(word):\n",
    "      sent.append(normalize_word(word))\n",
    "  sent = ' '.join(sent)\n",
    "  sent = word_tokenize(sent)\n",
    "  sent = ' '.join(sent)\n",
    "  doc = nlp(sent)\n",
    "  sent = [(X.text, X.label_) for X in doc.ents]\n",
    "  return sent\n",
    "\n",
    "\n",
    "data_hw5 = [df_hw5.values[i][1] for i in range(len(df_hw5.values))]\n",
    "messages_hw5 = [x for x in data_hw5]\n",
    "message_entities_hw5 = []\n",
    "for message in tqdm.tqdm(messages_hw5):\n",
    "  message_entities_hw5.append(normalize_sent(str(message)))\n",
    "\n",
    "total_entities_hw5 = set()\n",
    "for entities in tqdm.tqdm(message_entities_hw5):\n",
    "  for entity in entities:\n",
    "    total_entities_hw5.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5dc3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hw5 = nx.Graph()\n",
    "G_hw5.add_nodes_from(total_entities_hw5)\n",
    "for entities in message_entities_hw5:\n",
    "  for i in range(len(entities)):\n",
    "    for j in range(i+1, len(entities)):\n",
    "      G_hw5.add_edge(entities[i], entities[j])\n",
    "\n",
    "def sort_tuples(tup):\n",
    "  lst = len(tup)\n",
    "  for i in range(0, lst):\n",
    "    for j in range(0, lst-i-1):\n",
    "      if (tup[j][1] < tup[j + 1][1]):\n",
    "        temp = tup[j]\n",
    "        tup[j]= tup[j + 1]\n",
    "        tup[j + 1]= temp\n",
    "  return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cc3b2",
   "metadata": {},
   "source": [
    "## Link Analysis\n",
    "### A) Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9678dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1549/1549 [00:00<00:00, 604145.15it/s]\n"
     ]
    }
   ],
   "source": [
    "pr_hw5 = nx.pagerank(G_hw5)\n",
    "entity_prob_hw5 = []\n",
    "\n",
    "for entity in total_entities_hw5:\n",
    "  entity_prob_hw5.append((entity, pr_hw5[entity]))\n",
    "\n",
    "categories_hw5 = ['NORP', 'ORG', 'DATE', 'PERSON', 'GPE', 'LOC', 'CARDINAL']\n",
    "entity_prob_hw5 = sort_tuples(entity_prob_hw5)\n",
    "entity_category_hw5 = {}\n",
    "for entity in tqdm.tqdm(entity_prob_hw5):\n",
    "  for category in categories_hw5:\n",
    "    if entity[0][1] == category:\n",
    "      if category not in entity_category_hw5:\n",
    "        entity_category_hw5[category] = []\n",
    "      if len(entity_category_hw5[category]) < 10:\n",
    "        entity_category_hw5[category].append(entity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3be1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_results():\n",
    "    results = []\n",
    "    for category in entity_category_hw5:\n",
    "        results.append(category)\n",
    "        results.append('_______')\n",
    "        for entity in entity_category_hw5[category]:\n",
    "            results.append(f'-------{entity}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7d9425",
   "metadata": {},
   "source": [
    "### A) HITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23fd941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_results(num_outputs=10):\n",
    "    results = []\n",
    "    h_hw5, a_hw5 = nx.hits(G_hw5)\n",
    "    hubs_hw5 = []\n",
    "    for hub in h_hw5:\n",
    "      hubs_hw5.append((hub, h_hw5[hub]))\n",
    "\n",
    "    hubs_hw5 = sort_tuples(hubs_hw5)\n",
    "\n",
    "    authorities_hw5 = []\n",
    "    for authority in a_hw5:\n",
    "      authorities_hw5.append((authority, a_hw5[authority]))\n",
    "\n",
    "    authorities_hw5 = sort_tuples(authorities_hw5)\n",
    "\n",
    "    results .append(f'Top {num_outputs} hubs: ')\n",
    "    results .append('_____')\n",
    "    for hub in hubs_hw5[:num_outputs]:\n",
    "        results.append('(' + hub[0][0] + ', ' + hub[0][1] + f' ) with score: {hub[1]}')\n",
    "        \n",
    "    results.append('‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú')\n",
    "    results.append('‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú‚¨ú')\n",
    "    results .append(f'Top {num_outputs} authorities: ')\n",
    "    results .append('_____')\n",
    "    for authority in authorities_hw5[:num_outputs]:\n",
    "        results.append('(' + authority[0][0] + ', ' + authority[0][1] + f' ) with score: {authority[1]}')\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b967a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fef0f7b",
   "metadata": {},
   "source": [
    "# Setting up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f7d5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, query_type, queryExp):\n",
    "    if query_type == 'classification':\n",
    "        return lr_results() #+ transformer_results()\n",
    "    elif query_type == 'clustering':\n",
    "        return clustering_results()\n",
    "    elif query_type == 'hits':\n",
    "        return hits_results(10)\n",
    "    elif query_type == 'page_rank':\n",
    "        return page_rank_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b470b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f81cc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "hostName = \"localhost\"\n",
    "serverPort = 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started http://localhost:8080\n",
      "{'query': '', 'queryExp': False, 'type': 'classification', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jul/2022 12:57:18] \"POST /classification/ HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "class MyServer(BaseHTTPRequestHandler):\n",
    "    def end_headers (self):\n",
    "        self.send_header('Access-Control-Allow-Origin', '*')\n",
    "        SimpleHTTPRequestHandler.end_headers(self)\n",
    "        \n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-type\", \"text/html\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(bytes(\"<html><head><title>https://pythonbasics.org</title></head>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>Request: %s</p>\" % self.path, \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>This is an example web server.</p>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
    "        \n",
    "    def do_POST(self):\n",
    "        # 1. How long was the message?\n",
    "        length = int(self.headers.get('Content-length', 0))\n",
    "\n",
    "        # 2. Read the correct amount of data from the request.\n",
    "        data = self.rfile.read(length).decode()\n",
    "\n",
    "        # 3. Extract the \"message\" field from the request data.\n",
    "        message = json.loads(data)['query']\n",
    "        print(json.loads(data))\n",
    "        response = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
    "        reply = {'answers': response, 'expanded_query': 'I\\'m an exmanded query and I\\'m very very happy panda! üêº  :D'}\n",
    "        \n",
    "        # Send the \"message\" field back as the response.        \n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/plain; charset=utf-8')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(reply).encode())\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    webServer = HTTPServer((hostName, serverPort), MyServer)\n",
    "    print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
    "\n",
    "    try:\n",
    "        webServer.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    webServer.server_close()\n",
    "    print(\"Server stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d44ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
