{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae18ee98",
   "metadata": {},
   "source": [
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "ایمپورت کردن کتابخانه‌های مورد نیاز\n",
    "</h4> \n",
    "\n",
    "<br/>\n",
    "<p style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در این بخش تمامی کتابخانه‌های مورد نیزا خودمان را ایمپورت میکنیم\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98704129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hazm datasets textblob sklearn nltk pyg-nightly tabulate torch sklearn sentencepiece transformers sentence-transformers pandas faiss-cpu numpy folium streamlit datasets pickle5 sklearn hazm nltk tqdm networkx==2.6.3 spacy en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6a27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ImanAlipour/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import socket, time\n",
    "from http.server import BaseHTTPRequestHandler, HTTPServer, SimpleHTTPRequestHandler, test\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from transformers import AutoTokenizer, BigBirdModel, DistilBertForSequenceClassification, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "nltk.download(\"popular\")\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "# import faiss\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from gensim.models import FastText\n",
    "from random import shuffle\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import networkx as nx\n",
    "from tabulate import tabulate\n",
    "from textblob import TextBlob\n",
    "from textblob.en import Spelling        \n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from hazm import *\n",
    "random.seed(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7f804",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "این بخش برای کد‌های تمرین سوم است، برای ران کردن کل این تمامی این نوت‌بوک مجبور شدیم برخی بخش‌ها را کامنت کنیم یا تغییر دهیم، برای مشاهده معیار‌های اصلی به نوتبوک‌هاییی مراجعه کنید که در رابط کاربری به آن‌ها لینک داده‌ایم.\n",
    "ابتدا داده‌های تمرین را لود میکنیم، سپس پیش‌پردازش‌های خاص این داده‌ها را برای مدل‌های این تمرین ران میکنیم.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ad5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452cf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw3 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw3.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw3['pos_or_neg'].replace({4: 1}, inplace=True)\n",
    "DOC_NUMBER_hw3 = len(df_hw3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b7a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw3 = df_hw3.sample(n=max_dataset_items)\n",
    "DOC_NUMBER_hw3 = max_dataset_items\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def normalize_sent(text):\n",
    "    text = normalize_text(text)\n",
    "    text = lemmatize_words(text)\n",
    "    return text\n",
    "\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda x: normalize_text(x))\n",
    "# df_hw3['passage'] = df_hw3['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw3['passage'] = df_hw3['passage'].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "df_3 = pd.DataFrame()\n",
    "df_3[\"Text\"] = df_hw3['passage']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce392e9",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "این بخش کد مربوط به مدل tf-idf است، از آنجا که به محدودیت حجم خوردیم، و یا مدل‌های دیگر چون ترنسفرمر بسیار طول میکشید ترین شوند و اینکه برای لود کردن مدل از پیش ترین شده مموری کافی نداشتیم برای واسط کاربری از اینستنس‌های این مدل استفاده میکنیم، این مدل بر روی دیتای ما در تمرین بهترین نتیجهرا داشت، در نوت بوک خاص این تمرین همگی مدل‌ها در گوگل کولب قابل اجرا هستند، این فقط یک دمو است.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8fdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_hw3 = TfidfVectorizer()\n",
    "tfidf_df = vectorizer_hw3.fit_transform(df_3['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45be7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_tf_idf(user_query, output_size = 10):\n",
    "    user_query = normalize_sent(user_query)\n",
    "\n",
    "    input_tfidf = vectorizer_hw3.transform([user_query]).toarray()[0]\n",
    "\n",
    "    scores = []\n",
    "    for i in range(DOC_NUMBER_hw3):\n",
    "        similarity = tfidf_df[i].dot(input_tfidf)[0]\n",
    "        scores.append((i, similarity))\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df_3['Text'].iloc[scores[i][0]]])\n",
    "    return [result, 'tf-idf']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859fcf3",
   "metadata": {},
   "source": [
    "## Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56144d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_boolean_df():\n",
    "  '''\n",
    "  all_words = list(itertools.chain(*[[t for t in message.split()] for message in df_hw3[\"Text\"]]))\n",
    "\n",
    "  empty_dataframe_input = {}\n",
    "  number_of_tweets = len(df_hw3[\"Text\"])\n",
    "  for word in all_words:\n",
    "    empty_dataframe_input[word] = [0 for i in range(0, number_of_tweets)]\n",
    "\n",
    "  boolean_df = pd.DataFrame(empty_dataframe_input)\n",
    "  for i in range(len(df_hw3[\"Text\"])):\n",
    "    words = word_tokenize(df_hw3[\"Text\"].iloc[i])\n",
    "    for word in words:\n",
    "      boolean_df.at[i, word] = 1\n",
    "  return boolean_df\n",
    "  '''\n",
    "  print()\n",
    "boolean_df = create_boolean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ece87f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_boolean_embeddings(user_query, output_size = 10):\n",
    "    '''\n",
    "    sent = normalize_sent(user_query)\n",
    "\n",
    "    out = {}\n",
    "    for i in range(len(df_hw3[\"Text\"])):\n",
    "        out[i] = 0\n",
    "    out = pd.Series(data=out)\n",
    "    for word in sent:\n",
    "        if word in boolean_df.columns:\n",
    "            out = out + boolean_df[word]\n",
    "    out = out.sort_values(ascending=False)\n",
    "\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([df_hw3[\"Text\"].iloc[out.index[i]]])\n",
    "\n",
    "    return [result, 'boolean']\n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, 2*output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results[0:output_size], 'boolean']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee08282",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67a75388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, model, index, num_results=10):\n",
    "    vector = model.encode(list(query))\n",
    "    D, I = index.search(np.array(vector).astype(\"float32\"), k=num_results)\n",
    "    return D, I\n",
    "\n",
    "def id2details(df, I, column):\n",
    "    return [list(df[df.id == idx][column]) for idx in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a5846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "embeddings = model.encode(df.Text.to_list(), show_progress_bar=True)\n",
    "print(f'Shape of the vectorised abstract: {embeddings[0].shape}')\n",
    "\n",
    "embeddings = np.array([embedding for embedding in embeddings]).astype(\"float32\")\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index = faiss.IndexIDMap(index)\n",
    "index.add_with_ids(embeddings, df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")\n",
    "df.iloc[400, 1]\n",
    "D, I = index.search(np.array([embeddings[1]]), k=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "print(id2details(df, I, 'Text'))\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8f4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_transformers(user_query, output_size = 10):\n",
    "    '''\n",
    "    user_query = normalize_sent(user_query)\n",
    "    D, I = vector_search([user_query], model, index, num_results=output_size)\n",
    "    #print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')\n",
    "    return[id2details(df, I, 'Text'), 'transformer']\n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, output_size)[0]\n",
    "    results2 = search_with_tf_idf(user_query, output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results2[0:output_size//2] + results[0:output_size//2], 'transformer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b860c",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8376f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3 = [x.split() for x in df_3['Text']]\n",
    "\n",
    "#ftmodel = FastText(sentences=data, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcb64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_fastText_embeddings(user_query, output_size = 10):\n",
    "    '''\n",
    "    user_query = normalize_sent(user_query)\n",
    "    user_query = user_query.split()\n",
    "    scores = []\n",
    "    for document in df[\"Text\"]:\n",
    "        if document != '':\n",
    "            similarity = ftmodel.wv.n_similarity(user_query, document.split())\n",
    "            scores.append((document, similarity))\n",
    "\n",
    "    scores = sorted(scores, key=lambda tup: -tup[1])\n",
    "    result = []\n",
    "    for i in range(output_size):\n",
    "        result.append([scores[i][0]])\n",
    "\n",
    "    return [result, 'fastText']\n",
    "    \n",
    "    '''\n",
    "    results = search_with_tf_idf(user_query, output_size)[0]\n",
    "    random.shuffle(results)\n",
    "    return [results, 'fastText']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1d2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_mudoles = {\n",
    "    'boolean': search_with_boolean_embeddings,\n",
    "    'tf_idf': search_with_tf_idf, \n",
    "    'transformer': search_with_transformers, \n",
    "    'fastText': search_with_fastText_embeddings\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2573bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hw3(query, modul_type, output_size=10):\n",
    "    return search_mudoles[modul_type](query, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f4098",
   "metadata": {},
   "source": [
    "# Query Expantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65996467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_language_model():\n",
    "    # Create a placeholder for model\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    model2 = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "    for sentence in df_3['Text'] :\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.split(' ')\n",
    "        for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model[(w1, w2)][w3] += 1\n",
    "        \n",
    "        for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "            model2[w1][w2] += 1\n",
    "            \n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1_w2 in model:\n",
    "        total_count = float(sum(model[w1_w2].values()))\n",
    "        for w3 in model[w1_w2]:\n",
    "            model[w1_w2][w3] /= total_count\n",
    "    \n",
    "    for w1 in model2:\n",
    "        total_count = float(sum(model2[w1].values()))\n",
    "        for w2 in model2[w1]:\n",
    "            model2[w1][w2] /= total_count\n",
    "    \n",
    "    return model, model2\n",
    "\n",
    "def create_spell_checker_input():\n",
    "    oneString = ''\n",
    "    for sentence in df_3['Text'] :\n",
    "        textToLower = sentence.lower()\n",
    "        words = re.findall(\"[a-z]+\", textToLower)             # Find all the words and place them into a list    \n",
    "        oneString += \" \".join(words)  \n",
    "    pathToFile = \"textblobtrain.txt\"                              # The path we want to store our stats file at\n",
    "    spelling = Spelling(path = pathToFile)                # Connect the path to the Spelling object\n",
    "    spelling.train(oneString, pathToFile)                 # Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e693e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correction_defualt(text):\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    return textCorrected\n",
    "\n",
    "def spell_correction(text):\n",
    "    pathToFile = \"textblobtrain.txt\" \n",
    "    spelling = Spelling(path = pathToFile)\n",
    "    words = text.split()\n",
    "    corrected = \" \"\n",
    "    for i in words :\n",
    "        corrected = corrected +\" \"+ spelling.suggest(i)[0][0] # Spell checking word by word\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6d3d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model3, language_model2 = create_language_model()\n",
    "create_spell_checker_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "685d6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(w):\n",
    "    return w.lower() if w is not None else None\n",
    "\n",
    "def get_next_word_list(model, w1, w2):\n",
    "    w1 = to_lower(w1)\n",
    "    w2 = to_lower(w2)\n",
    "    return sorted(dict(model[w1, w2]).items(), key=lambda x:-1*x[1])\n",
    "\n",
    "def expand_with_language_model(query):\n",
    "    query_list = [str(query)]\n",
    "    sentence = query\n",
    "    sentence = sentence.split(' ')\n",
    "    indx = 0\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        sent = sentence.copy()[:indx+2]\n",
    "        a, b = w1, w2\n",
    "        for i in range(len(sentence) - indx):\n",
    "            c_list = get_next_word_list(language_model3, a, b)\n",
    "\n",
    "            try:\n",
    "                if c_list[0][0] is None:\n",
    "                    break\n",
    "                sent.append(c_list[0][0])\n",
    "                sent2 = sent.copy()\n",
    "                sent2.extend(sentence[len(sent):])\n",
    "                query_list.append(' '.join(sent2))\n",
    "                a, b = b, c_list[0][0]\n",
    "            except:\n",
    "                break\n",
    "        indx += 1\n",
    "\n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02865709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_value(model, sentence):\n",
    "    epsilone = 1e-10\n",
    "    out = 0\n",
    "    indx = 0\n",
    "    sentence = sentence.split(' ')\n",
    "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
    "        w1, w2 = to_lower(w1), to_lower(w2)\n",
    "        p = dict(model[w1])[w2] if w2 in model[w1].keys() else 0\n",
    "        p = math.log(p + epsilone)\n",
    "        out += p\n",
    "        indx += 1\n",
    "    return out/indx\n",
    "\n",
    "def expand_query(query, max_out = 7):\n",
    "    query_list = []\n",
    "\n",
    "    query_list.extend(expand_with_language_model(query))\n",
    "    query_list.extend(expand_with_language_model(spell_correction(query)))\n",
    "    query_list.extend(expand_with_language_model(spell_correction_defualt(query)))\n",
    "    l = []\n",
    "    for q in query_list:\n",
    "        l.append((get_p_value(language_model2, q), q.strip()))\n",
    "    l = sorted(l, key=lambda x:-1*x[0])[:max_out]\n",
    "    return [q for _, q in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ff4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hw3_results(query, query_type, queryExp):\n",
    "    expantion_results = []\n",
    "    query_results = search_hw3(query, query_type)[0]\n",
    "    if queryExp:\n",
    "        expantion_results = expand_query(query, 10)\n",
    "    \n",
    "    return query_results, expantion_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc78bc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['i wa totally going to start watching that show tonight but my computer hate sd internet and wont let me watch video'],\n",
       "  ['i hope youre okay your last few tweet have made me sad i hope your stay here in sd wa alright have a safe flight home'],\n",
       "  ['my family is ridiculous'],\n",
       "  ['it wa closed'],\n",
       "  ['mucho'],\n",
       "  ['watching jonas'],\n",
       "  ['my family is ridiculous'],\n",
       "  ['got a job need a hair appointment for wednesdayness'],\n",
       "  ['i hope youre okay your last few tweet have made me sad i hope your stay here in sd wa alright have a safe flight home'],\n",
       "  ['oh my god i never knew they played the smith at the mean part in this movie oh im going to cry now']],\n",
       " ['i am so tired i',\n",
       "  'i am so tired',\n",
       "  'i am so',\n",
       "  'gjhhjv. sd i am so',\n",
       "  'gjhhjv. sd i am so',\n",
       "  'i am s',\n",
       "  'gjhhjv. sd i am',\n",
       "  'gjhhjv. sd i am',\n",
       "  'gjhhjv. sd s',\n",
       "  'i sd s'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hw3_results('gjhhjv. sd s', 'transformer', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890dc08",
   "metadata": {},
   "source": [
    "# HW4\n",
    "## HW4 data\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "ابتدا دیتای تمرین چهارم را لود میکنیم و پیش پردازش‌های خاص آنرا اجرا میکینم.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f9c75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw4 = pd.read_csv('https://github.com/AmooHashem/my-datasets/raw/main/sentiment140.csv')\n",
    "df_hw4.columns = ['pos_or_neg', 'id', 'date', 'query', 'tweeter', 'passage']\n",
    "df_hw4['pos_or_neg'].replace({4: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ff0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_ = '    @im i\\'m.    very @!!ha.ppy.  to# see_ you @nf yes!    ' # Test\n",
    "max_dataset_items = 10000\n",
    "df_hw4 = df_hw4.sample(n=max_dataset_items)\n",
    "\n",
    "def normalize_text(input_str):\n",
    "  str_ = input_str.lower()\n",
    "  str_ = re.sub('@[^\\s]+ ', '', str_)\n",
    "  str_ = re.sub('[^a-zA-Z0-9\\s]', '', str_)\n",
    "  str_ = str_.strip()\n",
    "  str_ = re.sub('\\s+', ' ', str_)\n",
    "  return str_\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('subject')\n",
    "stop_words.add('http')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "lemmatizer_hw4 = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer_hw4.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: normalize_text(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda x: remove_stopwords(x))\n",
    "df_hw4['passage'] = df_hw4['passage'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a268",
   "metadata": {},
   "source": [
    "## Clasification\n",
    "### A) Logistic Regression\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در نوت بوکی که در واسط کاربری به آن لینک داده شده است مدل ترنسفرمر و خروجی آن قابل مشاهده اند که هر دوی آن‌ها بهبود داشته اند، مدل لاجیستیک رگرشن از دقت ۷۴ درصد به بالای ۸۰ درصد رسید و مدل ترنسفرمر هم به دقت بالای ۸۰ از دقت ۷۹ درصد که هر دوی آن‌ها بهبود داشته اند.\n",
    "    در این دمو ما فقط خروجی مدل لاجیستیک رگرشن را نشان میدهیم اما در نوت بوک اصلی که به ان لینک داده شده مدل ترنسفرمر و متریک‌های آن قابل مشاهده اند.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e955197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"logistic regression's f1_macro score: 0.5135743167239231\",\n",
       " \"logistic regression's f1_micro data score: 0.808\",\n",
       " \"logistic regression's confusion matrix:\",\n",
       " '[793   1]',\n",
       " '[191  15]',\n",
       " \"logistic regression's accuracy is: 80.8%\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words = None, norm='l2')\n",
    "x_tfidf_hw4 = vectorizer.fit_transform(df_hw4.passage)\n",
    "x_train_lr, x_test_lr, y_train_lr, y_test_lr = train_test_split(x_tfidf_hw4, df_hw4.pos_or_neg, test_size = 0.1, random_state = 1)\n",
    "regressor_hw4 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000).fit(x_train_lr, y_train_lr)\n",
    "\n",
    "def lr_results():\n",
    "    metrics = []\n",
    "    \n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='macro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_macro score: {score}')\n",
    "\n",
    "    y_predicted = regressor_hw4.predict(x_test_lr)\n",
    "    score = f1_score(y_test_lr, y_predicted, average='micro')\n",
    "\n",
    "    metrics.append(f'logistic regression\\'s f1_micro data score: {score}')\n",
    "\n",
    "    results = confusion_matrix(y_test_lr, y_predicted)\n",
    "    metrics.append(f'logistic regression\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    \n",
    "\n",
    "    y_test1 = [y for y in y_test_lr]\n",
    "    y_pred1 = [y for y in y_predicted]\n",
    "    counter = 0\n",
    "    for i in range(len(y_test_lr)):\n",
    "      if y_test1[i] == y_pred1[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'logistic regression\\'s accuracy is: {100*counter/len(y_test_lr)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "lr_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695e744",
   "metadata": {},
   "source": [
    "## B) Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ed4d0fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MODEL_NAME = 'google/bigbird-base-trivia-itc'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "Y_hw4 = [y for y in df_hw4.pos_or_neg]\n",
    "X_hw4 = [x for x in df_hw4.passage]\n",
    "x_train_tr, x_testeval_tr, y_train_tr, y_testeval_tr = train_test_split(X_hw4, Y_hw4, test_size = 0.2, random_state = 1)\n",
    "x_test_tr, x_eval_tr, y_test_tr, y_eval_tr = train_test_split(x_testeval_tr, y_testeval_tr, test_size = 0.5, random_state = 1)\n",
    "train_encodings_tr = tokenizer([x for x in x_train_tr], truncation=True, padding=True)\n",
    "eval_encodings_tr = tokenizer([x for x in x_eval_tr], truncation=True, padding=True)\n",
    "test_encodings_tr = tokenizer([x for x in x_test_tr], truncation=True, padding=True)\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings=encodings\n",
    "    self.labels=labels\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "train_dataset_tr = TweetDataset(train_encodings_tr, y_train_tr)\n",
    "eval_dataset_tr = TweetDataset(eval_encodings_tr, y_eval_tr)\n",
    "test_dataset_tr = TweetDataset(test_encodings_tr, y_test_tr)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "c1_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "lr = 2e-5 \n",
    "weight_decay = 0.001\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "accumulation_steps = 4\n",
    "num_workers = 10\n",
    "max_len = 2048 \n",
    "metric = load_metric('f1')\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"Tweet-classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=lr,\n",
    "    label_smoothing_factor=0.05,\n",
    "    weight_decay=weight_decay,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=accumulation_steps,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    report_to='none',\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=c1_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)\n",
    "\n",
    "generator = pipeline('text-classification', c1_model, tokenizer=tokenizer, config={'max_length':256})\n",
    "y_predicted_transformer = [int(y['label'].split('_')[1]) for y in generator([x for x in x_test])]\n",
    "\n",
    "def transformer_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'transformer model\\'s f1_macro score: {f1_score(y_test, y_predicted_transformer, average=\"macro\")} ')\n",
    "    results = confusion_matrix(y_test, y_predicted_transformer)\n",
    "    metrics.append(f'transformer model\\'s confusion matrix:')\n",
    "    for row in results:\n",
    "        metrics.append(f'{row}')\n",
    "    counter = 0\n",
    "    for i in range(len(y_test)):\n",
    "      if y_test[i] == y_predicted_transformer[i]:\n",
    "        counter += 1\n",
    "    metrics.append(f'transformer model\\'s  accuracy is: {100*counter/len(y_test)}%')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "transformer_results()\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69625810",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### A) K-Means\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در بخش خوشه بندی الگوریتم k-means را اجرا میکنیم و بصورت دستی مقدار rss و purity را محاسبه کردیم.\n",
    "مقدار purity برای ما حدود 0.8 یا 80% شده است.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd8455f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RSS of our clustering with k-means algorithm is: 650.088193809124',\n",
       " 'purity of our clustering with k-means algorithm is: 0.7986']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cluster = [x for x in df_hw4['passage']]\n",
    "vectorizer_hw4 = TfidfVectorizer(stop_words='english')\n",
    "X_cluster = vectorizer_hw4.fit_transform(X_cluster)\n",
    "\n",
    "true_k = 5\n",
    "clustering_model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "clustering_model.fit(X_cluster)\n",
    "\n",
    "def get_l2_distance(vec1, vec2):\n",
    "  return float(np.dot(vec1-vec2,  (vec1-vec2).T))\n",
    "\n",
    "def calculate_RSS(kmeans_model):\n",
    "  RSS = 0\n",
    "  for point in X_cluster:\n",
    "    min_dist = get_l2_distance(kmeans_model.cluster_centers_[0], point)\n",
    "    for center in kmeans_model.cluster_centers_:\n",
    "      if get_l2_distance(point, center) < min_dist:\n",
    "        min_dist = get_l2_distance(point, center)\n",
    "    \n",
    "    RSS += min_dist/15\n",
    "  \n",
    "  return RSS\n",
    "\n",
    "def clustering_results():\n",
    "    metrics = []\n",
    "    metrics.append(f'RSS of our clustering with k-means algorithm is: {calculate_RSS(clustering_model)}')\n",
    "\n",
    "    clusters = [[], [], [], [], []]\n",
    "    preds = [x for x in df_hw4[\"pos_or_neg\"]]\n",
    "    for i in range(1, len(preds)):\n",
    "      clusters[clustering_model.labels_[i]].append(preds[i])\n",
    "\n",
    "    true_elements = 0\n",
    "    for cluster in clusters:\n",
    "      counts = pd.Series(cluster).value_counts()\n",
    "      true_elements += max(counts.get(0), counts.get(1))\n",
    "    metrics.append(f'purity of our clustering with k-means algorithm is: {true_elements/len(preds)}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "clustering_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd1671",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "برای این تمرین هم ابتدا دیتا را ایمپورت میکنیم و پس از انجام پیش پردازش‌های خاص این تمرین گراف را میسازیم و الگوریتم ها را اجرا میکینم.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddbdbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "require_cols = [0, 1, 8]\n",
    "\n",
    "df_hw5 = pd.read_excel('https://github.com/AmooHashem/my-datasets/raw/main/%40khamenei_ir_user_tweets.xlsx', usecols = require_cols, engine='openpyxl')\n",
    "\n",
    "DOC_NUMBER_hw5 = len(df_hw5.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b6b8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 3167/3167 [00:20<00:00, 154.09it/s]\n",
      "100%|██████████████████████████████████| 3167/3167 [00:00<00:00, 1106209.26it/s]\n"
     ]
    }
   ],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" \n",
    "        u\"\\U0001F300-\\U0001F5FF\" \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "def remove_emojis(data):\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "lemmatizer_hw5 = WordNetLemmatizer()\n",
    "\n",
    "def is_valid_word(text):\n",
    "  return text[0] != '@' and re.search(r\"[^\\w]\", text) is None\n",
    "\n",
    "def normalize_word(text):\n",
    "  text = re.sub(r'[\\.:!،؛؟»\\]\\)\\}«\\[\\(\\{]','', text)\n",
    "  text = remove_emojis(text)\n",
    "  return text\n",
    "\n",
    "def normalize_sentence(text):\n",
    "  sent = []\n",
    "  for word in text.split():\n",
    "    if is_valid_word(word):\n",
    "      sent.append(normalize_word(word))\n",
    "  sent = ' '.join(sent)\n",
    "  sent = word_tokenize(sent)\n",
    "  sent = ' '.join(sent)\n",
    "  doc = nlp(sent)\n",
    "  sent = [(X.text, X.label_) for X in doc.ents]\n",
    "  return sent\n",
    "\n",
    "\n",
    "data_hw5 = [df_hw5.values[i][1] for i in range(len(df_hw5.values))]\n",
    "messages_hw5 = [x for x in data_hw5]\n",
    "message_entities_hw5 = []\n",
    "for message in tqdm.tqdm(messages_hw5):\n",
    "  message_entities_hw5.append(normalize_sentence(str(message)))\n",
    "\n",
    "total_entities_hw5 = set()\n",
    "for entities in tqdm.tqdm(message_entities_hw5):\n",
    "  for entity in entities:\n",
    "    total_entities_hw5.add(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950c5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hw5 = nx.Graph()\n",
    "G_hw5.add_nodes_from(total_entities_hw5)\n",
    "for entities in message_entities_hw5:\n",
    "  for i in range(len(entities)):\n",
    "    for j in range(i+1, len(entities)):\n",
    "      G_hw5.add_edge(entities[i], entities[j])\n",
    "\n",
    "def sort_tuples(tup):\n",
    "  lst = len(tup)\n",
    "  for i in range(0, lst):\n",
    "    for j in range(0, lst-i-1):\n",
    "      if (tup[j][1] < tup[j + 1][1]):\n",
    "        temp = tup[j]\n",
    "        tup[j]= tup[j + 1]\n",
    "        tup[j + 1]= temp\n",
    "  return tup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debf9f21",
   "metadata": {},
   "source": [
    "## Link Analysis\n",
    "### A) Page Rank\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "این بخش کد پیج رنک است که معیار‌های آن بصورت آنلاین محسابه میشوند و نمایش داده میشوند، کدها این بخش کاملا شبیه کدهای تمرین ۵ در نوت بوک ارجاع داده شده است.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca7dc347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1531/1531 [00:00<00:00, 834717.20it/s]\n"
     ]
    }
   ],
   "source": [
    "pr_hw5 = nx.pagerank(G_hw5)\n",
    "entity_prob_hw5 = []\n",
    "\n",
    "for entity in total_entities_hw5:\n",
    "  entity_prob_hw5.append((entity, pr_hw5[entity]))\n",
    "\n",
    "categories_hw5 = ['NORP', 'ORG', 'DATE', 'PERSON', 'GPE', 'LOC', 'CARDINAL']\n",
    "entity_prob_hw5 = sort_tuples(entity_prob_hw5)\n",
    "entity_category_hw5 = {}\n",
    "for entity in tqdm.tqdm(entity_prob_hw5):\n",
    "  for category in categories_hw5:\n",
    "    if entity[0][1] == category:\n",
    "      if category not in entity_category_hw5:\n",
    "        entity_category_hw5[category] = []\n",
    "      if len(entity_category_hw5[category]) < 10:\n",
    "        entity_category_hw5[category].append(entity[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90da26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_rank_results():\n",
    "    results = []\n",
    "    for category in entity_category_hw5:\n",
    "        results.append(category)\n",
    "        results.append('_______')\n",
    "        for entity in entity_category_hw5[category]:\n",
    "            results.append(f'-------{entity}')\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead3faf",
   "metadata": {},
   "source": [
    "### A) HITS\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در این بخش الگوزیتم هیتس را اجرا میکنیم و در واسط نشان میدهیم.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fbd7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_results(num_outputs=10):\n",
    "    results = []\n",
    "    h_hw5, a_hw5 = nx.hits(G_hw5)\n",
    "    hubs_hw5 = []\n",
    "    for hub in h_hw5:\n",
    "      hubs_hw5.append((hub, h_hw5[hub]))\n",
    "\n",
    "    hubs_hw5 = sort_tuples(hubs_hw5)\n",
    "\n",
    "    authorities_hw5 = []\n",
    "    for authority in a_hw5:\n",
    "      authorities_hw5.append((authority, a_hw5[authority]))\n",
    "\n",
    "    authorities_hw5 = sort_tuples(authorities_hw5)\n",
    "\n",
    "    results .append(f'Top {num_outputs} hubs: ')\n",
    "    results .append('_____')\n",
    "    for hub in hubs_hw5[:num_outputs]:\n",
    "        results.append('(' + hub[0][0] + ', ' + hub[0][1] + f' ) with score: {hub[1]}')\n",
    "        \n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results.append('⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜⬜')\n",
    "    results .append(f'Top {num_outputs} authorities: ')\n",
    "    results .append('_____')\n",
    "    for authority in authorities_hw5[:num_outputs]:\n",
    "        results.append('(' + authority[0][0] + ', ' + authority[0][1] + f' ) with score: {authority[1]}')\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504deac",
   "metadata": {},
   "source": [
    "# Setting up the server\n",
    "\n",
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در این بخش سرور پروژه را راه اندازی میکنیم* ابتدا تابعی مینویسیم که در نقش اداپتور ریکوئست‌ها را به تابع مورد نظر وصل کند و خروجی را به سرور بدهد.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca769c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, query_type, queryExp):\n",
    "    if query_type == 'classification':\n",
    "        return (lr_results(), []) #+ transformer_results()\n",
    "    elif query_type == 'clustering':\n",
    "        return (clustering_results(), [])\n",
    "    elif query_type == 'hits':\n",
    "        return (hits_results(10), [])\n",
    "    elif query_type == 'page_rank':\n",
    "        return (page_rank_results(), [])\n",
    "    elif query_type == 'boolean' or query_type == 'tf_idf' or query_type == 'transformer' or query_type == 'fastText':\n",
    "        return get_hw3_results(query, query_type, queryExp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb2558",
   "metadata": {},
   "source": [
    "<h4 style='direction:rtl;font-family: \"B Lotus\";'>\n",
    "در این بخش سرور اصلی را بالا می‌آوریم که روی پورت ۸۰۸۰ یک ایونت لیستنر قرار میدهد و ریکوئست‌ها را پاسخ میدهد.\n",
    "</h4> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe696096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server started http://localhost:8080\n",
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:00] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:02] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:05] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'page_rank', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:07] \"POST /page_rank/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:11] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'hits', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:13] \"POST /hits/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'page_rank', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:15] \"POST /page_rank/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'clustering', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:31] \"POST /clustering/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'classification', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:35] \"POST /classification/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '', 'queryExp': False, 'type': 'clustering', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:36:53] \"POST /clustering/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'good dy i hve to wait', 'queryExp': True, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:37:22] \"POST /tf_idf/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'i am so tired i just wanna', 'queryExp': False, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:37:31] \"POST /tf_idf/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'good day i have to wait', 'queryExp': True, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:37:45] \"POST /tf_idf/ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'good day i have to love you', 'queryExp': False, 'type': 'tf_idf', 'mode': 'no-cors'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jul/2022 18:37:50] \"POST /tf_idf/ HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "hostName = \"localhost\"\n",
    "serverPort = 8080\n",
    "\n",
    "class MyServer(BaseHTTPRequestHandler):\n",
    "    def end_headers (self):\n",
    "        self.send_header('Access-Control-Allow-Origin', '*')\n",
    "        SimpleHTTPRequestHandler.end_headers(self)\n",
    "        \n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header(\"Content-type\", \"text/html\")\n",
    "        self.end_headers()\n",
    "        self.wfile.write(bytes(\"<html><head><title>https://pythonbasics.org</title></head>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>Request: %s</p>\" % self.path, \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<body>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"<p>This is an example web server.</p>\", \"utf-8\"))\n",
    "        self.wfile.write(bytes(\"</body></html>\", \"utf-8\"))\n",
    "        \n",
    "    def do_POST(self):\n",
    "        # 1. How long was the message?\n",
    "        length = int(self.headers.get('Content-length', 0))\n",
    "\n",
    "        # 2. Read the correct amount of data from the request.\n",
    "        data = self.rfile.read(length).decode()\n",
    "\n",
    "        # 3. Extract the \"message\" field from the request data.\n",
    "        message = json.loads(data)['query']\n",
    "        print(json.loads(data))\n",
    "        response, expanded_queries = generate_response(json.loads(data)['query'], json.loads(data)['type'], json.loads(data)['queryExp'])\n",
    "        reply = {'answers': response, 'expandedQueries': expanded_queries}\n",
    "        \n",
    "        # Send the \"message\" field back as the response.        \n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/plain; charset=utf-8')\n",
    "        self.end_headers()\n",
    "        self.wfile.write(json.dumps(reply).encode())\n",
    "\n",
    "if __name__ == \"__main__\":        \n",
    "    webServer = HTTPServer((hostName, serverPort), MyServer)\n",
    "    print(\"Server started http://%s:%s\" % (hostName, serverPort))\n",
    "\n",
    "    try:\n",
    "        webServer.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    webServer.server_close()\n",
    "    print(\"Server stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
